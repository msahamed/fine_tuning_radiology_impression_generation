{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc6c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfbfcd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe_for_arrow(df):\n",
    "      \"\"\"\n",
    "      Clean DataFrame to make it compatible with PyArrow conversion\n",
    "      \"\"\"\n",
    "      print(\"Cleaning DataFrame for PyArrow compatibility...\")\n",
    "\n",
    "      # Keep only the essential columns for training\n",
    "      essential_columns = [\n",
    "          'report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality',\n",
    "          'training_input', 'training_output', 'clean_findings', 'clean_impression',\n",
    "          'clean_history', 'clean_technique'\n",
    "      ]\n",
    "\n",
    "      # Filter to only columns that exist in the dataframe\n",
    "      available_columns = [col for col in essential_columns if col in df.columns]\n",
    "      df_clean = df[available_columns].copy()\n",
    "\n",
    "      # Convert all columns to string type to avoid Arrow conversion issues\n",
    "      for col in df_clean.columns:\n",
    "          if col not in ['report_id']:  # Keep report_id as is if it's numeric\n",
    "              df_clean[col] = df_clean[col].astype(str)\n",
    "              # Replace 'nan' strings with None\n",
    "              df_clean[col] = df_clean[col].replace(['nan', 'None', ''], None)\n",
    "\n",
    "      # Fill any remaining NaN values\n",
    "      df_clean = df_clean.fillna('')\n",
    "\n",
    "      print(f\"Cleaned DataFrame shape: {df_clean.shape}\")\n",
    "      print(f\"Columns retained: {list(df_clean.columns)}\")\n",
    "\n",
    "      return df_clean\n",
    "\n",
    "def load_processed_data_to_huggingface():\n",
    "    \"\"\"\n",
    "    Convert processed radiology data to HuggingFace Dataset format\n",
    "    \"\"\"\n",
    "\n",
    "    # Define paths\n",
    "    data_dir = \"./processed_data\"\n",
    "\n",
    "    # Load the processed datasets\n",
    "    print(\"Loading processed data...\")\n",
    "    train_df = pd.read_csv(f\"{data_dir}/train_data.csv\", low_memory=False)\n",
    "    val_df = pd.read_csv(f\"{data_dir}/val_data.csv\", low_memory=False)\n",
    "    test_df = pd.read_csv(f\"{data_dir}/test_data.csv\", low_memory=False)\n",
    "\n",
    "    # Clean DataFrames for Arrow compatibility\n",
    "    train_df = clean_dataframe_for_arrow(train_df)\n",
    "    val_df = clean_dataframe_for_arrow(val_df)\n",
    "    test_df = clean_dataframe_for_arrow(test_df)\n",
    "\n",
    "    # Load reference banks for style learning\n",
    "    with open(f\"{data_dir}/reference_banks.json\", 'r') as f:\n",
    "        reference_banks = json.load(f)\n",
    "\n",
    "    print(f\"Loaded datasets:\")\n",
    "    print(f\"  Train: {len(train_df)} samples\")\n",
    "    print(f\"  Validation: {len(val_df)} samples\")\n",
    "    print(f\"  Test: {len(test_df)} samples\")\n",
    "    print(f\"  Reference banks: {len(reference_banks)} clinics\")\n",
    "\n",
    "    # Radiology-specific system message\n",
    "    system_message = \"\"\"You are an expert radiologist assistant specializing in generating accurate and concise medical impressions from radiology\n",
    "findings.\n",
    "\n",
    "Your task is to:\n",
    "1. **Analyze the findings**: Carefully review all clinical findings, history, and technique information\n",
    "2. **Generate focused impressions**: Create clear, prioritized conclusions that directly address the clinical question\n",
    "3. **Maintain clinical accuracy**: Ensure all significant findings are appropriately characterized\n",
    "4. **Use appropriate medical terminology**: Follow standard radiological reporting conventions\n",
    "5. **Adapt communication style**: Match the institutional reporting style and level of detail expected\n",
    "\n",
    "Generate only the IMPRESSION section based on the provided clinical information.\"\"\"\n",
    "\n",
    "    def create_radiology_chat_format(sample):\n",
    "        \"\"\"Convert radiology data to chat format with clinic/modality context\"\"\"\n",
    "\n",
    "        # Extract fields - handle potential None values\n",
    "        clinic = sample.get('mapped_clinic_id', 'unknown')\n",
    "        modality = sample.get('grouped_modality', 'unknown')\n",
    "        findings_input = sample.get('training_input', '')\n",
    "        impression_output = sample.get('training_output', '')\n",
    "        clinic_modality = sample.get('clinic_modality', f\"{clinic}_{modality}\")\n",
    "\n",
    "        # Skip samples with empty essential fields\n",
    "        if not findings_input or not impression_output:\n",
    "            return {\n",
    "                \"messages\": [],\n",
    "                \"clinic_id\": clinic,\n",
    "                \"modality\": modality,\n",
    "                \"findings\": findings_input,\n",
    "                \"impression\": impression_output,\n",
    "                \"clinic_modality\": clinic_modality,\n",
    "                \"is_valid\": False\n",
    "            }\n",
    "\n",
    "        # Create user message with clinical context\n",
    "        user_content = f\"Please generate an appropriate radiology impression for this {modality} study from {clinic}:\\n\\n{findings_input}\"\n",
    "\n",
    "        # Create chat messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": impression_output}\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"clinic_id\": clinic,\n",
    "            \"modality\": modality,\n",
    "            \"findings\": findings_input,\n",
    "            \"impression\": impression_output,\n",
    "            \"clinic_modality\": clinic_modality,\n",
    "            \"is_valid\": True\n",
    "        }\n",
    "\n",
    "    # Convert DataFrames to HuggingFace Datasets\n",
    "    print(\"Converting to HuggingFace Dataset format...\")\n",
    "\n",
    "    try:\n",
    "        train_dataset = Dataset.from_pandas(train_df)\n",
    "        val_dataset = Dataset.from_pandas(val_df)\n",
    "        test_dataset = Dataset.from_pandas(test_df)\n",
    "        print(\"✅ DataFrame to Dataset conversion successful!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in DataFrame conversion: {e}\")\n",
    "        print(\"Attempting alternative conversion method...\")\n",
    "\n",
    "        # Alternative: Convert to dict first\n",
    "        train_dataset = Dataset.from_dict(train_df.to_dict('list'))\n",
    "        val_dataset = Dataset.from_dict(val_df.to_dict('list'))\n",
    "        test_dataset = Dataset.from_dict(test_df.to_dict('list'))\n",
    "        print(\"✅ Alternative conversion successful!\")\n",
    "\n",
    "    # Apply chat formatting\n",
    "    print(\"Applying chat formatting...\")\n",
    "    train_dataset = train_dataset.map(create_radiology_chat_format, batched=False)\n",
    "    val_dataset = val_dataset.map(create_radiology_chat_format, batched=False)\n",
    "    test_dataset = test_dataset.map(create_radiology_chat_format, batched=False)\n",
    "\n",
    "    # Filter out invalid samples\n",
    "    train_dataset = train_dataset.filter(lambda x: x['is_valid'])\n",
    "    val_dataset = val_dataset.filter(lambda x: x['is_valid'])\n",
    "    test_dataset = test_dataset.filter(lambda x: x['is_valid'])\n",
    "\n",
    "    # Remove the is_valid column\n",
    "    train_dataset = train_dataset.remove_columns(['is_valid'])\n",
    "    val_dataset = val_dataset.remove_columns(['is_valid'])\n",
    "    test_dataset = test_dataset.remove_columns(['is_valid'])\n",
    "\n",
    "    # Create DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "\n",
    "    print(\"Dataset conversion complete!\")\n",
    "    print(f\"Final dataset structure:\")\n",
    "    print(dataset_dict)\n",
    "\n",
    "    return dataset_dict, reference_banks\n",
    "\n",
    "def apply_chat_template_to_dataset(dataset_dict, tokenizer, max_length=2048):\n",
    "    \"\"\"\n",
    "    Apply tokenizer chat template to convert messages to model input format\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"Convert chat messages to tokenized format\"\"\"\n",
    "\n",
    "        # Apply chat template to convert messages to text\n",
    "        texts = []\n",
    "        for messages in examples[\"messages\"]:\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "\n",
    "        # Tokenize the formatted text\n",
    "        tokenized = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=False,  # We'll pad during training\n",
    "            max_length=max_length,\n",
    "            return_overflowing_tokens=False,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"text\": texts\n",
    "        }\n",
    "\n",
    "    print(\"Applying chat template and tokenization...\")\n",
    "\n",
    "    # Apply tokenization to all splits\n",
    "    tokenized_datasets = dataset_dict.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"messages\"],  # Remove original messages, keep metadata\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "\n",
    "    print(\"Tokenization complete!\")\n",
    "    return tokenized_datasets\n",
    "\n",
    "def prepare_style_reference_data(reference_banks, tokenizer):\n",
    "    \"\"\"\n",
    "    Prepare reference data for style learning during training\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preparing style reference data...\")\n",
    "\n",
    "    style_references = {}\n",
    "\n",
    "    for clinic_id, impressions in reference_banks.items():\n",
    "        # Tokenize reference impressions for this clinic\n",
    "        tokenized_refs = tokenizer(\n",
    "            impressions,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,  # Shorter for impressions\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        style_references[clinic_id] = {\n",
    "            \"impressions\": impressions,\n",
    "            \"tokenized\": tokenized_refs\n",
    "        }\n",
    "\n",
    "    print(f\"Style references prepared for {len(style_references)} clinics\")\n",
    "    return style_references\n",
    "\n",
    "# Usage example for your notebook:\n",
    "def load_radiology_datasets():\n",
    "    \"\"\"\n",
    "    Main function to load and prepare all radiology datasets for training\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model name (same as in original notebook)\n",
    "    model_name = \"microsoft/MediPhi-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # Ensure tokenizer has pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load and convert data\n",
    "    dataset_dict, reference_banks = load_processed_data_to_huggingface()\n",
    "\n",
    "    # Apply chat template and tokenization\n",
    "    tokenized_datasets = apply_chat_template_to_dataset(dataset_dict, tokenizer)\n",
    "\n",
    "    # Prepare style references\n",
    "    style_references = prepare_style_reference_data(reference_banks, tokenizer)\n",
    "\n",
    "    # Save processed datasets for later use\n",
    "    tokenized_datasets.save_to_disk(\"./radiology_datasets\")\n",
    "\n",
    "    with open(\"./style_references.json\", 'w') as f:\n",
    "        # Save just the text data, not tokenized tensors\n",
    "        text_only_refs = {k: v[\"impressions\"] for k, v in style_references.items()}\n",
    "        json.dump(text_only_refs, f, indent=2)\n",
    "\n",
    "    print(\"All datasets saved successfully!\")\n",
    "\n",
    "    return tokenized_datasets, style_references, tokenizer\n",
    "\n",
    "# Sample the data to verify format\n",
    "def sample_dataset(dataset_dict, n_samples=2):\n",
    "    \"\"\"\n",
    "    Display sample data to verify correct formatting\n",
    "    \"\"\"\n",
    "    print(\"=== SAMPLE TRAINING DATA ===\")\n",
    "\n",
    "    for i in range(min(n_samples, len(dataset_dict[\"train\"]))):\n",
    "        sample = dataset_dict[\"train\"][i]\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Clinic: {sample['clinic_id']}\")\n",
    "        print(f\"Modality: {sample['modality']}\")\n",
    "        print(f\"Text preview: {sample['text'][:200]}...\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87990540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Cleaning DataFrame for PyArrow compatibility...\n",
      "Cleaned DataFrame shape: (18742, 10)\n",
      "Columns retained: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique']\n",
      "Cleaning DataFrame for PyArrow compatibility...\n",
      "Cleaned DataFrame shape: (4019, 10)\n",
      "Columns retained: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique']\n",
      "Cleaning DataFrame for PyArrow compatibility...\n",
      "Cleaned DataFrame shape: (4032, 10)\n",
      "Columns retained: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique']\n",
      "Loaded datasets:\n",
      "  Train: 18742 samples\n",
      "  Validation: 4019 samples\n",
      "  Test: 4032 samples\n",
      "  Reference banks: 6 clinics\n",
      "Converting to HuggingFace Dataset format...\n",
      "✅ DataFrame to Dataset conversion successful!\n",
      "Applying chat formatting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d894da02e4349a08ea985cb307c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e659ddb3aa4996b3c1509dcb1a0aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88c25923c644e46bb6736e14486a7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dfc20b7fd043709e43b393f94c1028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ed074ce6114d5b9bb6b440b105c91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cc1782fc5e4c16934f4e12ce115d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conversion complete!\n",
      "Final dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique', 'messages', 'clinic_id', 'modality', 'findings', 'impression'],\n",
      "        num_rows: 18742\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique', 'messages', 'clinic_id', 'modality', 'findings', 'impression'],\n",
      "        num_rows: 4019\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique', 'messages', 'clinic_id', 'modality', 'findings', 'impression'],\n",
      "        num_rows: 4032\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_dict, reference_banks = load_processed_data_to_huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ea580f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique', 'messages', 'clinic_id', 'modality', 'findings', 'impression'],\n",
       "        num_rows: 18742\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique', 'messages', 'clinic_id', 'modality', 'findings', 'impression'],\n",
       "        num_rows: 4019\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['report_id', 'mapped_clinic_id', 'grouped_modality', 'clinic_modality', 'training_input', 'training_output', 'clean_findings', 'clean_impression', 'clean_history', 'clean_technique', 'messages', 'clinic_id', 'modality', 'findings', 'impression'],\n",
       "        num_rows: 4032\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5ba233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b9679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
