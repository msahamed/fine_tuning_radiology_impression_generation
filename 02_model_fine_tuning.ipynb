{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df07268-c9ef-4b37-b45d-d4d601966c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # # # # Install Hugging Face libraries\n",
    "# %pip install  --upgrade \\\n",
    "#   \"evaluate\" \\\n",
    "#   \"tensorboard\" \\\n",
    "#   \"flash-attn\" \\\n",
    "#   \"liger-kernel\" \\\n",
    "#   \"setuptools\" \\\n",
    "#   \"deepspeed\" \\\n",
    "#   \"lm-eval[api]\" \\\n",
    "#   \"torch\"\\\n",
    "#   \"torchvision\" \\\n",
    "#   \"transformers\" \\\n",
    "#   \"datasets\" \\\n",
    "#   \"accelerate\" \\\n",
    "#   \"bitsandbytes\" \\\n",
    "#   \"trl\" \\\n",
    "#   \"peft\" \\\n",
    "#   \"lighteval\" \\\n",
    "#   \"hf-transfer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022cb1e-ffe6-4d5b-bfee-1d2b5d1339b9",
   "metadata": {},
   "source": [
    "### Import libraries and frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c51ba00-cba9-4157-a7f5-3b9fd9191a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, BitsAndBytesConfig\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import is_liger_kernel_available\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, TrlParser, ModelConfig, SFTConfig, get_peft_config\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# Use BitsAndBytesConfig for quantization that helps to reduce model size\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.optimizers import create_lorafa_optimizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54966395-e2da-44cd-9d37-aa38baf10f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Fixes the warning\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Clear cache before training\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cb671",
   "metadata": {},
   "source": [
    "### Check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c19ae2-45c8-4a2c-80af-baf75d28704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if hasattr(torch, \"accelerator\") else \"cuda\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b6344-c617-49f4-8f50-efeef94395f7",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da97daff-7f82-4b64-b825-9d7a8265c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = \"./data/processed/radiology_datasets\"\n",
    "dataset = load_from_disk(processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be810355-87f6-45fe-b786-cd62ce856b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['findings', 'impression', 'text', 'clinic_id', 'modality', 'clinic_modality'],\n",
       "        num_rows: 8865\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['findings', 'impression', 'text', 'clinic_id', 'modality', 'clinic_modality'],\n",
       "        num_rows: 1901\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['findings', 'impression', 'text', 'clinic_id', 'modality', 'clinic_modality'],\n",
       "        num_rows: 1915\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba1aa8-01ca-4ac3-ac82-79ce01198d47",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01c6b4c-de32-48cf-8bf1-5470fb7f735e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d415ce5f041947458678dce2e2e94937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. APPLY QUANTIZATION (This was missing!)\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 for better memory efficiency\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Load model WITH quantization\n",
    "model_name = \"microsoft/MediPhi-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,  \n",
    "    dtype=torch.bfloat16,      # Use bfloat16 for memory efficiency\n",
    "    device_map=\"auto\",               # Automatically distribute across GPUs\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"  # Ensure consistent padding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7e74d-69e1-4025-a72e-cbd2e158521b",
   "metadata": {},
   "source": [
    "### Understand model architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c11530-6238-4985-9920-c4a068c06e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d669f8c2-fe83-473a-b921-6b33a38110b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 197200896 || all params: 2009140224 || trainable%: 9.82%\n"
     ]
    }
   ],
   "source": [
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "print(\n",
    "    f\"trainable params: {trainable_params} || \"\n",
    "    f\"all params: {all_param} || \"\n",
    "    f\"trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77585eda-865a-40df-b8fe-dbb2902cbb81",
   "metadata": {},
   "source": [
    "### Before training test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9632c1d1-684f-4b6e-aed7-62a4e21252f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, StoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60c0bb10-01ba-4472-a77a-eb4f65579d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings: [CLINIC: clinic_1] [MODALITY: MR] FINDINGS: No abnormality along the sacral plexus presacral. Left sciatic nerve is normal in the greater sciatic foramen to the mid thigh. There is no mass or compression or edema along this nerve. Beginning distal mid thigh axial 36 and 35, there is intense edema of the peroneal branch of the sciatic nerve extending to the inferior margin lateral femoral condyle level left side only. There is no soft tissue mass or cyst along this nerve. No notable edema at the level of the fibular head. There is muscle denervation edema anterior and peroneal muscle compartments of the proximal leg left side only. No abnormality along the tibiofibular joint and specifically no marginating cyst. There is no other muscle denervation edema. The tibial branch of this left sciatic nerve is normal. Remaining muscles are normal. Left hamstring origin is intact with no marginating inflammation. Sacrum and sacroiliac joints are normal. Pubic symphysis has no marginating edema or effusion. Left pubic rami are normal. There is healed fracture with deformity right superior pubic ramus. There is also mild deformity from prior healed fracture of the right inferior pubic ramus. No marginating edema. Left hip joint has no effusion compared to moderate effusion on the right side. No subchondral edema of either hip joint. There is chondral thinning superomedial right notably greater than left hip joint and lateral marginal chondral thinning fissuring overall mild left side. No acute chondral injury or geographic defect. No paralabral cyst. No fluid-filled labral detachment. No intra-articular body or synovial thickening left hip. There is small subcortical cyst anterolateral femoral head neck junction. Tendons about both hip joints are intact with no bursitis or tendinitis. No abnormality along the iliotibial tract. No abnormality along the femoral shafts. No abnormality along the left femoral neurovascular bundle. IMPRESSION:\n",
      "\n",
      "Impressions: 1. Long segment intense neuritis with edema peroneal branch left sciatic nerve beginning distal mid femoral shaft and extending to the knee joint margin. Associated denervation edema anterior and peroneal muscle compartments of the proximal left leg. No mass or cyst or compression along this nerve. No other abnormality along the left sciatic nerve.\n"
     ]
    }
   ],
   "source": [
    "findings = dataset['test'][0]['findings']\n",
    "impression = dataset['test'][0]['impression']\n",
    "\n",
    "print(f\"Findings: {findings}\\n\")\n",
    "print(f\"Impressions: {impression}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b8145c-069f-48cb-849d-d0cfcc3104a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 32007: '<|end|>'\n"
     ]
    }
   ],
   "source": [
    "# Check what token ID 32007 represents\n",
    "print(f\"Token 32007: '{tokenizer.decode([32007])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62375c13-c37d-4fac-80cf-b15eac101e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  IMPRESSION:\n",
      "\n",
      "The MR imaging reveals intense edema of the peroneal branch of the left sciatic nerve extending to the inferior margin of the lateral femoral condyle, with no associated soft tissue mass or cyst. There is also muscle denervation edema in the anterior and peroneal muscle compartments of the proximal left leg. The tibiofibular joint and the left hip joint are normal, with no evidence of effusion or subchondral edema. The right superior pubic ramus shows a healed fracture with deformity, and the right inferior pubic ramus has mild deformity from a prior healed fracture. The right hip joint has moderate effusion, while the left hip joint has no effusion. There is mild chondral thinning on the right side of the hip joint, with no acute chondral\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/microsoft/MediPhi-Instruct\n",
    "# Radiology-specific system message\n",
    "system_message = \"\"\"You are an expert radiologist assistant specializing in generating accurate and concise medical impressions from radiology\n",
    "       findings.\n",
    "    \n",
    "      Your task is to:\n",
    "      1. **Analyze the findings**: Carefully review all clinical findings, history, and technique information\n",
    "      2. **Generate focused impressions**: Create clear, prioritized conclusions that directly address the clinical question\n",
    "      3. **Maintain clinical accuracy**: Ensure all significant findings are appropriately characterized\n",
    "      4. **Use appropriate medical terminology**: Follow standard radiological reporting conventions\n",
    "      5. **Adapt communication style**: Match the institutional reporting style and level of detail expected\n",
    "    \n",
    "      Generate only the IMPRESSION section based on the provided clinical information.\"\"\"\n",
    "\n",
    "# Hugging Face pipeline for text generation does apply apply_chat_template under the hood. \n",
    "# So we do not need to process for the text generation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": findings},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "#  stops generation when the model generates token ID 32007\n",
    "class EosListStoppingCriteria(StoppingCriteria):\n",
    "  def __init__(self, eos_sequence = [32007]):\n",
    "      self.eos_sequence = eos_sequence\n",
    "\n",
    "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "      last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
    "      return self.eos_sequence in last_ids\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "    \"stopping_criteria\": [EosListStoppingCriteria()]\n",
    "\n",
    "}\n",
    "output = pipe(messages, **generation_args)\n",
    "print(f\"AI: {output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec004dc0-c044-4160-b18e-5c2574522134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Long segment intense neuritis with edema peroneal branch left sciatic nerve beginning distal mid femoral shaft and extending to the knee joint margin. Associated denervation edema anterior and peroneal muscle compartments of the proximal left leg. No mass or cyst or compression along this nerve. No other abnormality along the left sciatic nerve.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{impression}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad53fb5f-c6b7-4300-980c-9383462cf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above testing, it is clear that Medphi is generating more or less similar text generation.\n",
    "# WIth fine tiuning the model might learn more numances of the dataset provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44810f-cbd2-460c-a656-eb3c4cb20bd2",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b156dea7-7b0b-4d93-8ce8-e5a632af68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    bias=\"none\",\n",
    "    target_modules = ['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17e74f48-1be7-4d50-834f-ebce68cf2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b41c979-04b5-4e00-bacc-952b68d6096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a03028a-85f9-4b0f-9c19-5093986f5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Safely preprocess logits with proper bounds checking\n",
    "    IMPORTANT: Must return tensors, not numpy arrays!\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    \n",
    "    # Convert logits to predicted token IDs\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Clamp token IDs to valid vocabulary range (keep as tensor)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    pred_ids = torch.clamp(pred_ids, 0, vocab_size - 1)\n",
    "    \n",
    "    # CRITICAL: Return tensors, not numpy arrays\n",
    "    return pred_ids, labels\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Robust compute_metrics function with error handling\n",
    "    \"\"\"\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    \n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(pred_ids, torch.Tensor):\n",
    "        pred_ids = pred_ids.cpu().numpy()\n",
    "    if isinstance(labels_ids, torch.Tensor):\n",
    "        labels_ids = labels_ids.cpu().numpy()\n",
    "    \n",
    "    # Ensure predictions are integers and within valid range\n",
    "    pred_ids = pred_ids.astype(np.int32)\n",
    "    labels_ids = labels_ids.astype(np.int32)\n",
    "    \n",
    "    # Clamp to vocabulary range to prevent overflow\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    pred_ids = np.clip(pred_ids, 0, vocab_size - 1)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to decode predictions\n",
    "        pred_str = []\n",
    "        for seq in pred_ids:\n",
    "            try:\n",
    "                # Additional safety: filter out any remaining invalid tokens\n",
    "                valid_seq = [int(token) for token in seq if 0 <= int(token) < vocab_size]\n",
    "                decoded = tokenizer.decode(valid_seq, skip_special_tokens=True)\n",
    "                pred_str.append(decoded)\n",
    "            except (OverflowError, ValueError) as e:\n",
    "                print(f\"Warning: Failed to decode sequence, using empty string. Error: {e}\")\n",
    "                pred_str.append(\"\")\n",
    "        \n",
    "        # Process labels\n",
    "        label_str = []\n",
    "        for seq in labels_ids:\n",
    "            try:\n",
    "                # Replace -100 with pad token\n",
    "                clean_seq = np.where(seq != -100, seq, tokenizer.pad_token_id)\n",
    "                # Ensure valid range\n",
    "                clean_seq = np.clip(clean_seq, 0, vocab_size - 1)\n",
    "                valid_seq = [int(token) for token in clean_seq]\n",
    "                decoded = tokenizer.decode(valid_seq, skip_special_tokens=True)\n",
    "                label_str.append(decoded)\n",
    "            except (OverflowError, ValueError) as e:\n",
    "                print(f\"Warning: Failed to decode label sequence, using empty string. Error: {e}\")\n",
    "                label_str.append(\"\")\n",
    "        \n",
    "        # Debug information\n",
    "        print(f\"Successfully decoded {len(pred_str)} predictions and {len(label_str)} labels\")\n",
    "        print(f\"Sample prediction: {pred_str[0][:100] if pred_str[0] else 'EMPTY'}...\")\n",
    "        print(f\"Sample label: {label_str[0][:100] if label_str[0] else 'EMPTY'}...\")\n",
    "        \n",
    "        # Compute ROUGE metrics\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        rouge_output = rouge.compute(\n",
    "            predictions=pred_str,\n",
    "            references=label_str,\n",
    "            rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"R1\": round(rouge_output[\"rouge1\"], 4),\n",
    "            \"R2\": round(rouge_output[\"rouge2\"], 4),\n",
    "            \"RL\": round(rouge_output[\"rougeL\"], 4),\n",
    "            \"RLsum\": round(rouge_output[\"rougeLsum\"], 4),\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in ROUGE computation: {e}\")\n",
    "        # Fallback to simple metrics\n",
    "        return simple_token_metrics(pred_ids, labels_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99aaac-6323-41dc-b6cf-c74c6f3e9a80",
   "metadata": {},
   "source": [
    "### Style Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e8090b-c448-4129-9349-337743b6807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_structured_format(text):\n",
    "    \"\"\"Check for numbered list (e.g., \"1. ... 2. ...\")\"\"\"\n",
    "    numbered_pattern = r\"^\\d+\\.\\s+.*?(?:\\n\\d+\\.\\s+.*?)*$\"\n",
    "    return bool(re.match(numbered_pattern, text, re.MULTILINE))\n",
    "\n",
    "def check_bullet_format(text):\n",
    "    \"\"\"Check for bullet points (e.g., \"- ... - ...\")\"\"\"\n",
    "    bullet_pattern = r\"^[-*‚Ä¢]\\s+.*?(?:\\n[-*‚Ä¢]\\s+.*?)*$\"\n",
    "    return bool(re.match(bullet_pattern, text, re.MULTILINE))\n",
    "\n",
    "# Safe preprocessing function (keeping what works)\n",
    "def safe_preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Safely preprocess logits - must return tensors!\n",
    "    \"\"\"\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    \n",
    "    # Convert logits to predicted token IDs\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Clamp token IDs to valid vocabulary range (keep as tensor)\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    pred_ids = torch.clamp(pred_ids, 0, vocab_size - 1)\n",
    "    \n",
    "    return pred_ids, labels\n",
    "\n",
    "def compute_style_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Simple compute metrics with just structured and bullet format checking\n",
    "    \"\"\"\n",
    "    try:\n",
    "        labels_ids = eval_pred.label_ids\n",
    "        pred_ids = eval_pred.predictions\n",
    "        \n",
    "        if isinstance(pred_ids, tuple):\n",
    "            pred_ids = pred_ids[0]\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(pred_ids, torch.Tensor):\n",
    "            pred_ids = pred_ids.cpu().numpy()\n",
    "        if isinstance(labels_ids, torch.Tensor):\n",
    "            labels_ids = labels_ids.cpu().numpy()\n",
    "        \n",
    "        # Ensure valid token IDs\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        pred_ids = np.clip(pred_ids, 0, vocab_size - 1)\n",
    "        \n",
    "        # Safe decoding with error handling\n",
    "        decoded_preds = []\n",
    "        decoded_labels = []\n",
    "        \n",
    "        for pred_seq in pred_ids:\n",
    "            try:\n",
    "                decoded = tokenizer.decode(pred_seq, skip_special_tokens=True)\n",
    "                decoded_preds.append(decoded)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to decode prediction: {e}\")\n",
    "                decoded_preds.append(\"\")\n",
    "        \n",
    "        for label_seq in labels_ids:\n",
    "            try:\n",
    "                # Replace -100 with pad token\n",
    "                clean_seq = np.where(label_seq != -100, label_seq, tokenizer.pad_token_id)\n",
    "                clean_seq = np.clip(clean_seq, 0, vocab_size - 1)\n",
    "                decoded = tokenizer.decode(clean_seq, skip_special_tokens=True)\n",
    "                decoded_labels.append(decoded)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to decode label: {e}\")\n",
    "                decoded_labels.append(\"\")\n",
    "        \n",
    "        # Calculate only the two style metrics you want\n",
    "        structured_count = sum(1 for pred in decoded_preds if check_structured_format(pred))\n",
    "        bullet_count = sum(1 for pred in decoded_preds if check_bullet_format(pred))\n",
    "        \n",
    "        # Simple metrics\n",
    "        metrics = {\n",
    "            \"structured_format_ratio\": structured_count / len(decoded_preds),\n",
    "            \"bullet_format_ratio\": bullet_count / len(decoded_preds),\n",
    "            \"num_samples\": len(decoded_preds),\n",
    "        }\n",
    "        \n",
    "        # Try to compute ROUGE (with fallback)\n",
    "        try:\n",
    "            rouge = evaluate.load(\"rouge\")\n",
    "            rouge_output = rouge.compute(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "                rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "            )\n",
    "            metrics.update({\n",
    "                \"rouge1\": round(rouge_output[\"rouge1\"], 4),\n",
    "                \"rouge2\": round(rouge_output[\"rouge2\"], 4),\n",
    "                \"rougeL\": round(rouge_output[\"rougeL\"], 4),\n",
    "            })\n",
    "            print(f\"‚úÖ Successfully computed metrics for {len(decoded_preds)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ROUGE computation failed: {e}\")\n",
    "            metrics[\"rouge_error\"] = str(e)[:50]\n",
    "        \n",
    "        # Debug sample outputs\n",
    "        if len(decoded_preds) > 0:\n",
    "            print(f\"üìù Sample prediction: {decoded_preds[0][:100]}...\")\n",
    "            print(f\"üìã Sample label: {decoded_labels[0][:100]}...\")\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Style metrics computation failed: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e)[:50],\n",
    "            \"num_samples\": 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb9c81eb-7bd7-4191-8072-70e55f16c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import os \n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    # Basic training parameters\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=30,\n",
    "    save_total_limit=2,\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "\n",
    "    # Evaluation settings\n",
    "    eval_accumulation_steps=1,  # Process eval in smaller chunks\n",
    "    prediction_loss_only=False,\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # assistant_only_loss=True,\n",
    "\n",
    "    # Memory and performance\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Mixed precision training\n",
    "    bf16=True if torch.cuda.is_bf16_supported() else False,\n",
    "    \n",
    "    # SFT-specific parameters\n",
    "    max_length=1024,\n",
    "    packing=True,  # Pack multiple short sequences into one\n",
    "    dataset_text_field=\"text\",\n",
    "    \n",
    "    # Gradient settings\n",
    "    max_grad_norm=0.3,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68b3465c-c232-41d0-89cf-0863144bf03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    peft_config=lora_config,\n",
    "    compute_metrics=compute_style_metrics,\n",
    "    preprocess_logits_for_metrics=safe_preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ba84bc5-2a46-416f-82c1-1690d76d1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='178' max='178' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [178/178 53:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Structured Format Ratio</th>\n",
       "      <th>Bullet Format Ratio</th>\n",
       "      <th>Num Samples</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.112000</td>\n",
       "      <td>0.640237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>614</td>\n",
       "      <td>0.802600</td>\n",
       "      <td>0.648300</td>\n",
       "      <td>0.760200</td>\n",
       "      <td>0.643234</td>\n",
       "      <td>828119.000000</td>\n",
       "      <td>0.846181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.551400</td>\n",
       "      <td>0.511141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>614</td>\n",
       "      <td>0.835400</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>0.504210</td>\n",
       "      <td>1651047.000000</td>\n",
       "      <td>0.873085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.509700</td>\n",
       "      <td>0.466889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>614</td>\n",
       "      <td>0.847200</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.811700</td>\n",
       "      <td>0.495526</td>\n",
       "      <td>2473316.000000</td>\n",
       "      <td>0.881857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.441129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>614</td>\n",
       "      <td>0.854000</td>\n",
       "      <td>0.724200</td>\n",
       "      <td>0.820400</td>\n",
       "      <td>0.442003</td>\n",
       "      <td>3297876.000000</td>\n",
       "      <td>0.887456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.430123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>614</td>\n",
       "      <td>0.857900</td>\n",
       "      <td>0.729600</td>\n",
       "      <td>0.824300</td>\n",
       "      <td>0.425535</td>\n",
       "      <td>4122039.000000</td>\n",
       "      <td>0.890209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully computed metrics for 614 samples\n",
      "üìù Sample prediction: You are an expert radiologist assistant specializing in generating accurate and concise medical impr...\n",
      "üìã Sample label: Áªô You are an expert radiologist assistant specializing in generating accurate and concise medical im...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully computed metrics for 614 samples\n",
      "üìù Sample prediction: You are an expert radiologist assistant specializing in generating accurate and concise medical impr...\n",
      "üìã Sample label: Áªô You are an expert radiologist assistant specializing in generating accurate and concise medical im...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully computed metrics for 614 samples\n",
      "üìù Sample prediction: You are an expert radiologist assistant specializing in generating accurate and concise medical impr...\n",
      "üìã Sample label: Áªô You are an expert radiologist assistant specializing in generating accurate and concise medical im...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully computed metrics for 614 samples\n",
      "üìù Sample prediction: You are an expert radiologist assistant specializing in generating accurate and concise medical impr...\n",
      "üìã Sample label: Áªô You are an expert radiologist assistant specializing in generating accurate and concise medical im...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully computed metrics for 614 samples\n",
      "üìù Sample prediction: You are an expert radiologist assistant specializing in generating accurate and concise medical impr...\n",
      "üìã Sample label: Áªô You are an expert radiologist assistant specializing in generating accurate and concise medical im...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7c550-d96c-4e8a-90b0-2e9603eedd63",
   "metadata": {},
   "source": [
    "### Save the adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42bb5700-870c-424c-b978-aa5ba5865dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lora_adapter(trainer, save_path=\"./lora_adapter\"):\n",
    "    \n",
    "    # Save the adapter\n",
    "    trainer.model.save_pretrained(save_path)\n",
    "    trainer.tokenizer.save_pretrained(save_path)\n",
    "    \n",
    "    print(f\"LoRA adapter saved to: {save_path}\")\n",
    "    print(f\"Adapter size: {get_directory_size(save_path):.2f} MB\")\n",
    "    \n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0c843e0-a69a-4b32-a977-b1ef9116360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09838978-5588-4619-82bf-3a90f4038235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter saved to: ./lora_adapter\n",
      "Adapter size: 51.97 MB\n"
     ]
    }
   ],
   "source": [
    "adapter_path = save_lora_adapter(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0d103-41a1-467c-9888-a64944d18e60",
   "metadata": {},
   "source": [
    "## Push to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e3598eb-9e10-47e7-ad7e-2e38a9881d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbc5687c-634f-47e0-a023-d3dc82850862",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f6b8423-1d2d-41bf-8b19-ba8dbfeb2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ea4b54b-8789-4901-9e1d-360067a823b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id=\"sabber/medphi-radiology-summary-adapter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14509f2a-f214-4b24-be1c-06876625cecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/sabber/medphi-radiology-summary-adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='sabber/medphi-radiology-summary-adapter')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_repo(repo_id=repo_id, token=token, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc80cb88-654e-4d9b-af61-9d24d2c158d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: adapter_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d86a6cf99f4988a5892bfc22c1fe85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361b64dde2684c5d876d3635f2c3ea89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: adapter_model.safetensors\n",
      "Uploaded: tokenizer.json\n",
      "Uploaded: tokenizer_config.json\n",
      "Uploaded: special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "files_to_upload = [\n",
    "    \"adapter_config.json\",\n",
    "    \"adapter_model.safetensors\",  # or adapter_model.bin\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\",\n",
    "    \"special_tokens_map.json\"\n",
    "]\n",
    "\n",
    "for file in files_to_upload:\n",
    "    file_path = os.path.join(adapter_path, file)\n",
    "    if os.path.exists(file_path):\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=file,\n",
    "            repo_id=repo_id,\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"Uploaded: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d756ebf-a972-4296-8b10-ef7d76571eef",
   "metadata": {},
   "source": [
    "### Test model with trained adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b46a21a1-78d3-4b7b-b053-e668d1b942fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lora_adapter',\n",
       " 'logs',\n",
       " 'results',\n",
       " '.ipynb_checkpoints',\n",
       " 'data',\n",
       " '00_eda.ipynb',\n",
       " '02_preprocess_.ipynb',\n",
       " '03_impression_model_ft.ipynb',\n",
       " '04_model_evaluation.ipynb',\n",
       " 'requirements.txt',\n",
       " 'processed_data.zip']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20fdce60-7576-40c9-8cde-451b704423bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.json',\n",
       " 'tokenizer.model',\n",
       " 'added_tokens.json',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer_config.json',\n",
       " 'chat_template.jinja',\n",
       " 'adapter_config.json',\n",
       " 'adapter_model.safetensors',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"./lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0e31aab-b6ef-46ce-b791-380a223cbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24a55581-b7a6-4d99-b4cf-a1e6224b0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. First, let's check the current adapter config\n",
    "# with open(\"./lora_adapter/adapter_config.json\", \"r\") as f:\n",
    "#     adapter_config = json.load(f)\n",
    "\n",
    "# print(\"Current adapter config:\")\n",
    "# print(adapter_config)\n",
    "\n",
    "# # 2. Add the missing base model path if it's not there\n",
    "# if \"base_model_name_or_path\" not in adapter_config or adapter_config[\"base_model_name_or_path\"] is None:\n",
    "#     adapter_config[\"base_model_name_or_path\"] = \"microsoft/MediPhi-Instruct\"\n",
    "    \n",
    "#     # Save the fixed config\n",
    "#     with open(\"./lora_adapter/adapter_config.json\", \"w\") as f:\n",
    "#         json.dump(adapter_config, f, indent=2)\n",
    "    \n",
    "#     print(\"‚úÖ Fixed adapter_config.json with base model path\")\n",
    "# else:\n",
    "#     print(\"‚úÖ Base model path already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dc89cd1-c207-412b-b85f-0f758c1ab25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import AutoPeftModelForCausalLM\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# ft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     \"./lora_adapter\",\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# ft_tokenizer = AutoTokenizer.from_pretrained(\"./lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c261c2-91bb-46ee-86a4-c7d3354b634b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
