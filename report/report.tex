\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}

% Code listing settings
\lstset{
    basicstyle=\ttfamily\footnotesize,
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\title{Automated Medical Impression Generation:\\
A Fine-Tuned Approach for Radiology Reports}
\author{Sabber Ahamed}
\date{\today}

\begin{document}

\maketitle

\section{Goal and Context}

The goal of this project is to develop a comprehensive approach to automate medical impression generation from radiology findings using fine-tuned large language models. I developed a findings-focused data processing pipeline that filters for substantial clinical content, implemented quality-based segmentation strategies, and fine-tuned Microsoft's \textbf{MediPhi-Instruct} model using LoRA based techniques. The resulting model demonstrates significant performance improvements over the base model across multiple imaging modalities, achieving a 19.6\% increase in ROUGE-1 scores and a 56.6\% increase in ROUGE-2 scores.

\section{Exploratory Data Analysis}

\subsection{Dataset Overview}
I began with 30,135 de-identified radiology reports spanning 6 clinics with diverse imaging modalities. Key findings from my EDA include:

\begin{itemize}
    \item \textbf{Modality Distribution}: MR imaging dominates (59.9\%, 18,046 reports), followed by CT (17.7\%, 5,322), CR (7.3\%, 2,205), US (7.1\%, 2,139), XR (2.9\%, 879), and NM/Other (1.1\%, 143)
    \item \textbf{Clinical Content Quality}: Only 46.8\% (14,091 reports) contain both substantial findings ($\geq$100 characters) and complete impressions
    \item \textbf{Clinic Variation}: Significant heterogeneity in reporting styles, with some clinics favoring structured numbered lists while others use paragraph format
\end{itemize}

I incorporated all of these insights into the prompt design and segmentation strategy.

\subsection{Data Quality Assessment}
Critical filtering revealed substantial data quality challenges:
\begin{itemize}
    \item 1,401 reports (4.6\%) missing basic metadata (clinic, modality, or structured content)
    \item 14,372 reports (47.7\%) with insufficient findings content ($<$100 characters)
    \item 1,387 reports (4.6\%) with inadequate impression sections
\end{itemize}

\subsection{Key Insights}
\begin{enumerate}
    \item \textbf{Findings-Rich Focus}: Reports with substantial findings ($\geq$100 characters) demonstrate significantly higher clinical value and impression quality
    \item \textbf{Modality-Specific Patterns}: Different imaging modalities exhibit distinct reporting conventions and complexity levels
    \item \textbf{Clinic Style Diversity}: Clear institutional preferences for formatting, terminology, and level of clinical detail
\end{enumerate}

\section{Segmentation Strategy and Justification}

\subsection{Evolution from Clinic-Based to Quality-Based Segmentation}

Initially, I pursued clinic-based segmentation to capture institutional reporting styles. However, discussing with Mark, I pivoted to quality-focused segmentation that better serves the core objective of generating medically accurate impressions.

\subsection{Implemented Segmentation Approach}

\subsubsection{Findings-Rich Filtering}
After discussing with Mark, I implemented a quality filter that prioritizes clinical substance. I used the filtered data for training:
\begin{itemize}
    \item \textbf{Minimum Findings Length}: 100 characters (ensures substantial clinical observations)
    \item \textbf{Impression Completeness}: 20-1000 characters
    \item \textbf{Essential Fields Only}: Focus on findings and impressions, excluding auxiliary metadata
\end{itemize}

On top of this filtering, I used modality and clinic id as segmenting data to fine tune and evaluate performance variations (See more details in evaluation section).

\subsection{Justification}

This quality-based filtering and modality-based approach offers several advantages:
\begin{itemize}
    \item \textbf{Clinical Relevance}: Emphasizes both medically meaningful and institutional preferences
    \item \textbf{Scalability}: Generalizes across institutions without overfitting to specific clinics
    \item \textbf{Efficiency}: Maximizes training value from high-quality examples
    \item \textbf{Performance}: Focuses model learning on clinically substantial cases
\end{itemize}

\section{Fine-Tuning Approach and Results}

\subsection{Model Selection}
I selected \textbf{Microsoft MediPhi-Instruct} as the base model due to its:
\begin{itemize}
    \item Medical domain pre-training and instruction-following capabilities
    \item Proven performance on clinical text generation tasks
    \item Efficient fine-tuning characteristics suitable for budget constraints
\end{itemize}

\subsection{Technical Implementation}
The fine-tuning approach leveraged efficient techniques to stay within the less than $\$$100 budget. I used runpod to manage the training on an RTX 4090 GPU instance.

\subsubsection{LoRA Configuration}
\begin{lstlisting}[language=Python]
lora_config = LoraConfig(
    r=8,                    # Low-rank dimension
    lora_alpha=32,          # Scaling parameter
    target_modules=['o_proj', 'qkv_proj',
                   'gate_up_proj', 'down_proj'],
    task_type="CAUSAL_LM"
)
\end{lstlisting}

\subsubsection{Training Parameters}
I have used the following training parameters. For more details of the code, please refer to \texttt{02\_model\_fine\_finetuning.ipynb} notebook.
\begin{itemize}
    \item \textbf{Dataset}: 8,865 training samples, 1,901 validation, 1,915 test
    \item \textbf{Batch Size}: 2 per device with 16 gradient accumulation steps
    \item \textbf{Learning Rate}: 2e-4 with cosine scheduler
    \item \textbf{Quantization}: 4-bit NF4 for memory efficiency
    \item \textbf{Max Length}: 1024 tokens with sequence packing
\end{itemize}

\subsection{Training Results}
The fine-tuning process completed successfully over 1 epoch with progressive improvement across 150 training steps. Final training metrics are presented in Table \ref{tab:training_results}.

\begin{table}[h]
\centering
\caption{Fine-Tuning Performance Metrics (Final Step: 150)}
\label{tab:training_results}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Trainable Parameters & 12.6M (0.33\% of total) \\
Training Loss (Final) & 0.4465 \\
Validation Loss (Final) & 0.4301 \\
ROUGE-1 & 0.8579 \\
ROUGE-2 & 0.7296 \\
ROUGE-L & 0.8243 \\
Structured Format Ratio & 0.0000 \\
Bullet Format Ratio & 0.0000 \\
Validation Samples & 614 \\
\bottomrule
\end{tabular}
\end{table}

The training demonstrated strong convergence, with training loss decreasing from 1.112 at step 30 to 0.4465 at step 150, while validation loss improved from 0.640 to 0.430, indicating effective learning without overfitting. ROUGE scores show excellent content overlap, with ROUGE-1 reaching 0.86 and ROUGE-L achieving 0.82, demonstrating strong semantic alignment with reference impressions.

\subsubsection{Style Metrics Analysis}
The style metrics (structured and bullet format ratios) currently show 0.0 values, indicating that the model is not generating impressions in the expected structured formats. This occurred due to:

\begin{enumerate}
    \item \textbf{Prompt Template Issues}: The model outputs include system messages rather than clean impressions. Due to time constraints, I could not refine the prompt templates further.
    \item \textbf{Tokenization Challenges}: The evaluation pipeline captures full model responses instead of extracted impressions. 
    \item \textbf{Format Detection Limitations}: Current regex patterns may not capture the variety of clinical formatting styles
\end{enumerate}

\subsubsection{Next Steps for Improvement}
\begin{itemize}
    \item \textbf{Output Parsing}: Implement robust impression extraction from model responses
    \item \textbf{Template Refinement}: Optimize chat templates to ensure clean impression-only outputs
    \item \textbf{Style Pattern Enhancement}: Expand format detection to include clinical paragraph styles
    \item \textbf{Evaluation Framework}: Develop comprehensive evaluation pipeline with proper text processing
\end{itemize}

\subsection{Model Deployment}
The fine-tuned adapter was successfully deployed to Hugging Face Hub:
\begin{center}
\texttt{https://huggingface.co/sabber/medphi-radiology-summary-adapter}
\end{center}

\section{Evaluation and Results}

\subsection{Systematic Modality-Based Evaluation}
I implemented a comprehensive evaluation framework that systematically tests both the base Microsoft MediPhi-Instruct model and my fine-tuned adapter across all imaging modalities. The evaluation uses 20 randomly sampled cases per modality from the test set (138 total samples).

\subsubsection{Evaluation Framework}
\begin{itemize}
    \item \textbf{Automated Metrics}: ROUGE-1, ROUGE-2, and ROUGE-L scores measuring content overlap with reference impressions
    \item \textbf{Systematic Sampling}: 20 samples per modality with random seed for reproducibility
    \item \textbf{Baseline Comparison}: Direct comparison between base model and fine-tuned adapter
    \item \textbf{Modality-Specific Analysis}: Performance assessment across 7 imaging modalities
\end{itemize}

\subsection{Comparative Evaluation Results}

\subsubsection{Overall Performance Improvement}
The fine-tuned model demonstrates significant improvement over the base model across all metrics:

\begin{table}[h]
\centering
\caption{Overall Model Comparison (138 test samples)}
\label{tab:overall_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Base Model} & \textbf{Fine-Tuned} \\
\midrule
ROUGE-1 & 0.3465 & 0.4146 (+19.6\%) \\
ROUGE-2 & 0.1800 & 0.2818 (+56.6\%) \\
ROUGE-L & 0.2727 & 0.3720 (+36.4\%) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Modality-Specific Performance}
Performance varies significantly across imaging modalities, as shown in Table \ref{tab:modality_comparison}:

\begin{table}[h]
\centering
\caption{ROUGE-1 Performance by Modality}
\label{tab:modality_comparison}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Modality} & \textbf{Base Model} & \textbf{Fine-Tuned} & \textbf{Improvement} \\
\midrule
MR & 0.4642 & 0.6274 & +0.1632 (+35.1\%) \\
Unspecified & 0.4186 & 0.5655 & +0.1469 (+35.1\%) \\
CR & 0.3283 & 0.3970 & +0.0687 (+20.9\%) \\
XR & 0.2859 & 0.3812 & +0.0953 (+33.3\%) \\
NM & 0.3440 & 0.2872 & -0.0568 (-16.5\%) \\
US & 0.3073 & 0.3394 & +0.0321 (+10.4\%) \\
CT & 0.2836 & 0.2978 & +0.0142 (+5.0\%) \\
OTHER & 0.2745 & 0.2276 & -0.0469 (-17.1\%) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Key Evaluation Insights}

\begin{enumerate}
    \item \textbf{Strongest Improvements}: MR imaging shows the largest absolute improvement (+0.1632 ROUGE-1), likely due to its prevalence in the training data (59.9\% of dataset)

    \item \textbf{Consistent Gains}: Six out of eight modalities show performance improvements, with four modalities achieving $>$20\% relative improvement

    \item \textbf{Challenging Modalities}: NM and OTHER modalities show performance decreases, possibly due to limited training examples and high variability

    \item \textbf{Clinical Relevance}: Major modalities (MR, CT, CR, XR) all demonstrate meaningful improvements, covering $>$85\% of clinical cases
\end{enumerate}

\section{Strategic Scaling Recommendations}
I had discussed couple of hypothesis during my interview about scaling. Of course with the growth of the business, we can not fine tune multiple model targeting different clinics or modalities. Here are some of the recommendations to scale this approach:
\begin{itemize}
    \item \textbf{Option-1: One unified model}: Train a single model that takes modality as an additional input feature, allowing it to adapt its generation style based on the specified imaging type. This reduces the need for multiple models while still capturing modality-specific nuances.
    \item \textbf{Option-2: Cluster-based style adaptation fine tuning}: In this option we can cluster clinics and radiologists reporting styles and fine-tune a model for each cluster. This balances the need for customization with scalability, as fewer models are required compared to clinic-specific fine-tuning.
    \item \textbf{Option-3: Data augmentation techniques}: Use data augmentation techniques to expand the training dataset, especially for underrepresented modalities. Synthetic data generation can help balance the dataset and improve model robustness.
\end{itemize}


\section{Conclusion}

This technical assessment demonstrates a practical, cost-effective approach to automated medical impression generation that achieves significant performance improvements over baseline models. By focusing on findings-rich content and implementing quality-based segmentation, I developed a robust fine-tuning pipeline that produces clinically relevant impressions while maintaining scalability and efficiency. The fine-tuned model shows strong gains across multiple imaging modalities, particularly in MR and CR, which are critical for clinical practice. While some challenges remain in output formatting and style consistency, the overall results validate the effectiveness of the approach.

\end{document}